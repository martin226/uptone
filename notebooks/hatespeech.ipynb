{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Classifier\n",
    "Classifies Tweets into (0 = hate speech, 1 = offensive language, 2 = neither) using a convolutional neural network trained on https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset.\n",
    "\n",
    "Accuracy: ~87%\n",
    "\n",
    "In order to handle false positives, sentiment analysis is used in addition to this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q demoji\n",
    "!pip install -q contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import demoji\n",
    "import contractions\n",
    "import string\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, concatenate\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True)\n",
    "\n",
    "stop_words = set(stopwords.words('english') + [\"rt\"])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "abbreviations = {\n",
    "    \"lol\": \"laughing\",\n",
    "    \"lmao\": \"laughing\",\n",
    "    \"lmfao\": \"laughing\",\n",
    "    \"kys\": \"kill yourself\",\n",
    "    \"stfu\": \"shut up\",\n",
    "    \"lmk\": \"let me know\",\n",
    "    \"nvm\": \"nevermind\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"bc\": \"because\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"cya\": \"see you\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"ftw\": \"for the win\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"gtg\": \"got to go\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"ily\": \"i love you\",\n",
    "    \"fs\": \"for sure\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"ong\": \"for real\",\n",
    "    \"fr\": \"for real\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "}\n",
    "\n",
    "\n",
    "def twitter_preprocess(text: str):\n",
    "    \"\"\"\n",
    "    Preprocessor for tweets\n",
    "    Remove punctuation\n",
    "    Remove hashtags\n",
    "    Remove stopwords\n",
    "    Remove URLs, hashtags, and usernames\n",
    "    Expand common abbreviations and contractions\n",
    "    Convert emojis to words\n",
    "    Convert to lowercase\n",
    "    Remove non-ASCII characters\n",
    "    Stem words\n",
    "    \"\"\"\n",
    "    tokens = tweet_tokenizer.tokenize(text)\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        # Remove punctuation\n",
    "        if token in string.punctuation:\n",
    "            continue\n",
    "        # Remove hashtags\n",
    "        if token.startswith('#'):\n",
    "            token = token[1:]\n",
    "        # Remove stopwords\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        # Remove urls\n",
    "        if token.startswith('http'):\n",
    "            continue\n",
    "        # Expand common abbreviations and contractions\n",
    "        token = contractions.fix(token)\n",
    "        for key, value in abbreviations.items():\n",
    "            token = re.sub(r'\\b' + key + r'\\b', value, token)\n",
    "        # Convert emojis to words\n",
    "        token = demoji.replace_with_desc(token, '')\n",
    "        # Remove non-ASCII characters\n",
    "        token = re.sub(r'[^\\x00-\\x7F]+', '', token)\n",
    "        # Lemmatize\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        if token != '':\n",
    "            filtered_tokens.append(token)\n",
    "    text = \" \".join(filtered_tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/labeled_data.csv')\n",
    "df = pd.concat([df['tweet'],df['class']], axis=1)\n",
    "df['tweet'] = df['tweet'].apply(twitter_preprocess)\n",
    "X = df['tweet'].fillna(\"Invalid\").values\n",
    "y = df['class'].values\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")\n",
    "max_features = 20000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Export tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=label_count)\n",
    "y_test = to_categorical(y_test, num_classes=label_count)\n",
    "\n",
    "hate, offensive, neither = np.bincount(df['class'])\n",
    "total = hate + offensive + neither\n",
    "\n",
    "class_weights = {0: (1 / hate)*(total)/3.0, 1: (1 / offensive)*(total)/3.0, 2: (1 / neither)*(total)/3.0}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = 3\n",
    "\n",
    "def build_model(conv_layers = 2, max_dilation_rate = 4):\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Conv1D(2*embed_size, kernel_size = 3)(x)\n",
    "    prefilt_x = Conv1D(2*embed_size, kernel_size = 3)(x)\n",
    "    out_conv = []\n",
    "\n",
    "    for dilation_rate in range(max_dilation_rate):\n",
    "        x = prefilt_x\n",
    "        for i in range(3):\n",
    "            x = Conv1D(32*2**(i), kernel_size = 3, dilation_rate = 2**dilation_rate)(x)    \n",
    "        out_conv += [Dropout(0.3)(GlobalMaxPooling1D()(x))]\n",
    "    x = concatenate(out_conv, axis = -1)    \n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(label_count, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"hatespeech.bin\"\n",
    "# Export best version of model\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=512,  \n",
    "          epochs=15,\n",
    "          callbacks=[checkpoint, early],\n",
    "          validation_data=(X_test, y_test),\n",
    "          class_weight=class_weights\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(file_path)\n",
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
